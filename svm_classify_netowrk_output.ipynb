{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specification\n",
    "\n",
    "## Project 1 -Predict secondary protein structure given the sequence. \n",
    "\n",
    "### Completion requirements:\n",
    "\n",
    "- Reimplement the network described by Qian and Sejnowski in 1988\n",
    "\n",
    "\n",
    "- Test and compare your accuracy - using their data\n",
    "\n",
    "\n",
    "- Implement a single improvement, such as profiling\n",
    "\n",
    "\n",
    "- Test and compare your accuracy again\n",
    "\n",
    "\n",
    "- Does the model get similar accuracy on unseen datasets?\n",
    "\n",
    "\n",
    "- Extend your work to other methods, e.g. can large language models help? How about SVMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a sequence of amino acids to a one-hot encoded matrix\n",
    "def one_hot_encode(seq, vocab):\n",
    "    # Create a matrix of zeros, with dimensions len(seq) x len(vocab)\n",
    "    one_hot = np.zeros((len(seq), len(vocab)), dtype=np.float32)\n",
    "    # Iterate over the sequence and set the appropriate elements to 1.0\n",
    "    for i, char in enumerate(seq):\n",
    "        if char in vocab:\n",
    "            one_hot[i, vocab.index(char)] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "# prepare data to enter neural network\n",
    "def prepare_data(filepath, window_size=13):\n",
    "    sequences = []\n",
    "    structures = []\n",
    "    current_seq = []\n",
    "    current_struct = []\n",
    "    processing_sequence = False  # Track when inside a sequence block\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == '<>': # Sequence separator\n",
    "                if processing_sequence: \n",
    "                    if current_seq and current_struct:\n",
    "                        seq_encoded = one_hot_encode(current_seq, aa_vocab) # One-hot encode sequence\n",
    "                        struct_encoded = one_hot_encode(current_struct, structure_vocab) # One-hot encode structure\n",
    "\n",
    "                        # Apply sliding window of determined size\n",
    "                        for i in range(len(seq_encoded) - window_size + 1):\n",
    "                            window = seq_encoded[i:i + window_size]\n",
    "                            label = struct_encoded[i + window_size // 2]\n",
    "                            sequences.append(window)\n",
    "                            structures.append(label)\n",
    "\n",
    "                    current_seq = []\n",
    "                    current_struct = []\n",
    "                processing_sequence = not processing_sequence\n",
    "                continue\n",
    "\n",
    "            elif 'end' in line:  # end of sequence or file\n",
    "                continue  \n",
    "            \n",
    "            # If inside a sequence block, process the sequence\n",
    "            # handles errors in sequence end lines in the dataset\n",
    "            if processing_sequence:\n",
    "                parts = line.split()\n",
    "                if len(parts) != 2:\n",
    "                    continue  \n",
    "                current_seq.append(parts[0])\n",
    "                current_struct.append(parts[1])\n",
    "\n",
    "    return np.array(sequences), np.array(structures)\n",
    "\n",
    "\n",
    "# Define your vocabularies\n",
    "aa_vocab = 'ACDEFGHIKLMNPQRSTVWY_'  # 20 amino acids + 1 for gap/unknown\n",
    "structure_vocab = 'he_'  # h for helix, e for sheet, _ for coil\n",
    "\n",
    "# Example paths, replace with your actual file pathsin\n",
    "train_path = 'Q_and_s_data/protein-secondary-structure.train.txt'\n",
    "test_path = 'Q_and_s_data/protein-secondary-structure.test.txt'\n",
    "\n",
    "x_train, y_train = prepare_data(train_path)\n",
    "x_test, y_test = prepare_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8110, 13, 21) (8110, 3)\n",
      "(1714, 13, 21) (1714, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4790 - loss: 0.2545 - val_accuracy: 0.5566 - val_loss: 0.2020\n",
      "Epoch 2/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - accuracy: 0.5313 - loss: 0.2054 - val_accuracy: 0.5589 - val_loss: 0.2006\n",
      "Epoch 3/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5325 - loss: 0.2035 - val_accuracy: 0.5595 - val_loss: 0.1993\n",
      "Epoch 4/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - accuracy: 0.5351 - loss: 0.2018 - val_accuracy: 0.5589 - val_loss: 0.1980\n",
      "Epoch 5/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - accuracy: 0.5365 - loss: 0.2002 - val_accuracy: 0.5583 - val_loss: 0.1968\n",
      "Epoch 6/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - accuracy: 0.5375 - loss: 0.1986 - val_accuracy: 0.5589 - val_loss: 0.1957\n",
      "Epoch 7/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - accuracy: 0.5385 - loss: 0.1971 - val_accuracy: 0.5583 - val_loss: 0.1946\n",
      "Epoch 8/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5399 - loss: 0.1957 - val_accuracy: 0.5583 - val_loss: 0.1936\n",
      "Epoch 9/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 0.5411 - loss: 0.1943 - val_accuracy: 0.5601 - val_loss: 0.1926\n",
      "Epoch 10/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - accuracy: 0.5426 - loss: 0.1930 - val_accuracy: 0.5607 - val_loss: 0.1916\n",
      "Epoch 11/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 852us/step - accuracy: 0.5439 - loss: 0.1918 - val_accuracy: 0.5613 - val_loss: 0.1907\n",
      "Epoch 12/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - accuracy: 0.5441 - loss: 0.1905 - val_accuracy: 0.5624 - val_loss: 0.1897\n",
      "Epoch 13/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850us/step - accuracy: 0.5475 - loss: 0.1894 - val_accuracy: 0.5618 - val_loss: 0.1889\n",
      "Epoch 14/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - accuracy: 0.5481 - loss: 0.1882 - val_accuracy: 0.5630 - val_loss: 0.1880\n",
      "Epoch 15/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - accuracy: 0.5495 - loss: 0.1871 - val_accuracy: 0.5648 - val_loss: 0.1872\n",
      "Epoch 16/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5516 - loss: 0.1860 - val_accuracy: 0.5653 - val_loss: 0.1864\n",
      "Epoch 17/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5537 - loss: 0.1850 - val_accuracy: 0.5648 - val_loss: 0.1856\n",
      "Epoch 18/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step - accuracy: 0.5555 - loss: 0.1840 - val_accuracy: 0.5653 - val_loss: 0.1849\n",
      "Epoch 19/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.5584 - loss: 0.1830 - val_accuracy: 0.5648 - val_loss: 0.1841\n",
      "Epoch 20/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - accuracy: 0.5599 - loss: 0.1820 - val_accuracy: 0.5653 - val_loss: 0.1834\n",
      "Epoch 21/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - accuracy: 0.5635 - loss: 0.1811 - val_accuracy: 0.5677 - val_loss: 0.1827\n",
      "Epoch 22/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - accuracy: 0.5657 - loss: 0.1802 - val_accuracy: 0.5700 - val_loss: 0.1820\n",
      "Epoch 23/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - accuracy: 0.5681 - loss: 0.1793 - val_accuracy: 0.5706 - val_loss: 0.1813\n",
      "Epoch 24/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - accuracy: 0.5699 - loss: 0.1784 - val_accuracy: 0.5735 - val_loss: 0.1807\n",
      "Epoch 25/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step - accuracy: 0.5721 - loss: 0.1776 - val_accuracy: 0.5770 - val_loss: 0.1801\n",
      "Epoch 26/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5758 - loss: 0.1768 - val_accuracy: 0.5823 - val_loss: 0.1795\n",
      "Epoch 27/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - accuracy: 0.5774 - loss: 0.1760 - val_accuracy: 0.5817 - val_loss: 0.1789\n",
      "Epoch 28/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - accuracy: 0.5793 - loss: 0.1752 - val_accuracy: 0.5817 - val_loss: 0.1783\n",
      "Epoch 29/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - accuracy: 0.5833 - loss: 0.1745 - val_accuracy: 0.5846 - val_loss: 0.1778\n",
      "Epoch 30/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - accuracy: 0.5862 - loss: 0.1738 - val_accuracy: 0.5863 - val_loss: 0.1772\n",
      "Epoch 31/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5878 - loss: 0.1731 - val_accuracy: 0.5863 - val_loss: 0.1767\n",
      "Epoch 32/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - accuracy: 0.5899 - loss: 0.1724 - val_accuracy: 0.5863 - val_loss: 0.1762\n",
      "Epoch 33/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - accuracy: 0.5915 - loss: 0.1717 - val_accuracy: 0.5863 - val_loss: 0.1757\n",
      "Epoch 34/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - accuracy: 0.5942 - loss: 0.1711 - val_accuracy: 0.5910 - val_loss: 0.1752\n",
      "Epoch 35/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step - accuracy: 0.5976 - loss: 0.1705 - val_accuracy: 0.5951 - val_loss: 0.1748\n",
      "Epoch 36/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - accuracy: 0.6001 - loss: 0.1699 - val_accuracy: 0.5957 - val_loss: 0.1743\n",
      "Epoch 37/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - accuracy: 0.6018 - loss: 0.1693 - val_accuracy: 0.5963 - val_loss: 0.1739\n",
      "Epoch 38/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step - accuracy: 0.6023 - loss: 0.1687 - val_accuracy: 0.6009 - val_loss: 0.1735\n",
      "Epoch 39/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - accuracy: 0.6030 - loss: 0.1682 - val_accuracy: 0.6021 - val_loss: 0.1731\n",
      "Epoch 40/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step - accuracy: 0.6043 - loss: 0.1677 - val_accuracy: 0.6033 - val_loss: 0.1728\n",
      "Epoch 41/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - accuracy: 0.6071 - loss: 0.1672 - val_accuracy: 0.6062 - val_loss: 0.1724\n",
      "Epoch 42/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - accuracy: 0.6079 - loss: 0.1667 - val_accuracy: 0.6109 - val_loss: 0.1721\n",
      "Epoch 43/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - accuracy: 0.6099 - loss: 0.1663 - val_accuracy: 0.6091 - val_loss: 0.1717\n",
      "Epoch 44/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6110 - loss: 0.1658 - val_accuracy: 0.6120 - val_loss: 0.1714\n",
      "Epoch 45/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - accuracy: 0.6120 - loss: 0.1654 - val_accuracy: 0.6120 - val_loss: 0.1711\n",
      "Epoch 46/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - accuracy: 0.6124 - loss: 0.1650 - val_accuracy: 0.6149 - val_loss: 0.1708\n",
      "Epoch 47/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - accuracy: 0.6148 - loss: 0.1646 - val_accuracy: 0.6161 - val_loss: 0.1706\n",
      "Epoch 48/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6168 - loss: 0.1642 - val_accuracy: 0.6173 - val_loss: 0.1703\n",
      "Epoch 49/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - accuracy: 0.6182 - loss: 0.1638 - val_accuracy: 0.6196 - val_loss: 0.1700\n",
      "Epoch 50/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - accuracy: 0.6196 - loss: 0.1635 - val_accuracy: 0.6231 - val_loss: 0.1698\n",
      "Epoch 51/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - accuracy: 0.6225 - loss: 0.1632 - val_accuracy: 0.6266 - val_loss: 0.1696\n",
      "Epoch 52/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896us/step - accuracy: 0.6237 - loss: 0.1628 - val_accuracy: 0.6272 - val_loss: 0.1694\n",
      "Epoch 53/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6265 - loss: 0.1625 - val_accuracy: 0.6272 - val_loss: 0.1692\n",
      "Epoch 54/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - accuracy: 0.6284 - loss: 0.1622 - val_accuracy: 0.6313 - val_loss: 0.1690\n",
      "Epoch 55/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - accuracy: 0.6287 - loss: 0.1620 - val_accuracy: 0.6330 - val_loss: 0.1688\n",
      "Epoch 56/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - accuracy: 0.6296 - loss: 0.1617 - val_accuracy: 0.6330 - val_loss: 0.1686\n",
      "Epoch 57/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - accuracy: 0.6304 - loss: 0.1614 - val_accuracy: 0.6319 - val_loss: 0.1684\n",
      "Epoch 58/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945us/step - accuracy: 0.6306 - loss: 0.1612 - val_accuracy: 0.6330 - val_loss: 0.1683\n",
      "Epoch 59/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - accuracy: 0.6310 - loss: 0.1609 - val_accuracy: 0.6336 - val_loss: 0.1681\n",
      "Epoch 60/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - accuracy: 0.6313 - loss: 0.1607 - val_accuracy: 0.6354 - val_loss: 0.1680\n",
      "Epoch 61/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - accuracy: 0.6317 - loss: 0.1605 - val_accuracy: 0.6342 - val_loss: 0.1678\n",
      "Epoch 62/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6337 - loss: 0.1603 - val_accuracy: 0.6336 - val_loss: 0.1677\n",
      "Epoch 63/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - accuracy: 0.6339 - loss: 0.1601 - val_accuracy: 0.6313 - val_loss: 0.1676\n",
      "Epoch 64/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - accuracy: 0.6343 - loss: 0.1599 - val_accuracy: 0.6313 - val_loss: 0.1675\n",
      "Epoch 65/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - accuracy: 0.6368 - loss: 0.1597 - val_accuracy: 0.6307 - val_loss: 0.1674\n",
      "Epoch 66/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - accuracy: 0.6376 - loss: 0.1595 - val_accuracy: 0.6319 - val_loss: 0.1673\n",
      "Epoch 67/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6384 - loss: 0.1594 - val_accuracy: 0.6319 - val_loss: 0.1672\n",
      "Epoch 68/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - accuracy: 0.6387 - loss: 0.1592 - val_accuracy: 0.6307 - val_loss: 0.1671\n",
      "Epoch 69/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - accuracy: 0.6396 - loss: 0.1591 - val_accuracy: 0.6307 - val_loss: 0.1670\n",
      "Epoch 70/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6394 - loss: 0.1589 - val_accuracy: 0.6307 - val_loss: 0.1669\n",
      "Epoch 71/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step - accuracy: 0.6413 - loss: 0.1588 - val_accuracy: 0.6319 - val_loss: 0.1668\n",
      "Epoch 72/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.6419 - loss: 0.1586 - val_accuracy: 0.6319 - val_loss: 0.1667\n",
      "Epoch 73/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - accuracy: 0.6421 - loss: 0.1585 - val_accuracy: 0.6330 - val_loss: 0.1667\n",
      "Epoch 74/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6407 - loss: 0.1584 - val_accuracy: 0.6336 - val_loss: 0.1666\n",
      "Epoch 75/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - accuracy: 0.6412 - loss: 0.1583 - val_accuracy: 0.6348 - val_loss: 0.1665\n",
      "Epoch 76/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - accuracy: 0.6412 - loss: 0.1582 - val_accuracy: 0.6342 - val_loss: 0.1665\n",
      "Epoch 77/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - accuracy: 0.6422 - loss: 0.1580 - val_accuracy: 0.6348 - val_loss: 0.1664\n",
      "Epoch 78/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6422 - loss: 0.1579 - val_accuracy: 0.6348 - val_loss: 0.1663\n",
      "Epoch 79/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - accuracy: 0.6438 - loss: 0.1578 - val_accuracy: 0.6342 - val_loss: 0.1663\n",
      "Epoch 80/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - accuracy: 0.6438 - loss: 0.1577 - val_accuracy: 0.6359 - val_loss: 0.1662\n",
      "Epoch 81/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - accuracy: 0.6445 - loss: 0.1576 - val_accuracy: 0.6342 - val_loss: 0.1662\n",
      "Epoch 82/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step - accuracy: 0.6458 - loss: 0.1576 - val_accuracy: 0.6330 - val_loss: 0.1661\n",
      "Epoch 83/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6462 - loss: 0.1575 - val_accuracy: 0.6330 - val_loss: 0.1661\n",
      "Epoch 84/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - accuracy: 0.6457 - loss: 0.1574 - val_accuracy: 0.6336 - val_loss: 0.1661\n",
      "Epoch 85/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step - accuracy: 0.6460 - loss: 0.1573 - val_accuracy: 0.6330 - val_loss: 0.1660\n",
      "Epoch 86/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - accuracy: 0.6449 - loss: 0.1572 - val_accuracy: 0.6319 - val_loss: 0.1660\n",
      "Epoch 87/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6451 - loss: 0.1572 - val_accuracy: 0.6313 - val_loss: 0.1660\n",
      "Epoch 88/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - accuracy: 0.6464 - loss: 0.1571 - val_accuracy: 0.6313 - val_loss: 0.1659\n",
      "Epoch 89/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step - accuracy: 0.6465 - loss: 0.1570 - val_accuracy: 0.6301 - val_loss: 0.1659\n",
      "Epoch 90/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step - accuracy: 0.6470 - loss: 0.1570 - val_accuracy: 0.6284 - val_loss: 0.1659\n",
      "Epoch 91/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6469 - loss: 0.1569 - val_accuracy: 0.6284 - val_loss: 0.1658\n",
      "Epoch 92/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - accuracy: 0.6472 - loss: 0.1568 - val_accuracy: 0.6289 - val_loss: 0.1658\n",
      "Epoch 93/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - accuracy: 0.6470 - loss: 0.1568 - val_accuracy: 0.6289 - val_loss: 0.1658\n",
      "Epoch 94/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - accuracy: 0.6471 - loss: 0.1567 - val_accuracy: 0.6295 - val_loss: 0.1657\n",
      "Epoch 95/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6474 - loss: 0.1567 - val_accuracy: 0.6301 - val_loss: 0.1657\n",
      "Epoch 96/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - accuracy: 0.6476 - loss: 0.1566 - val_accuracy: 0.6301 - val_loss: 0.1657\n",
      "Epoch 97/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - accuracy: 0.6477 - loss: 0.1566 - val_accuracy: 0.6301 - val_loss: 0.1657\n",
      "Epoch 98/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.6479 - loss: 0.1565 - val_accuracy: 0.6301 - val_loss: 0.1656\n",
      "Epoch 99/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6481 - loss: 0.1565 - val_accuracy: 0.6284 - val_loss: 0.1656\n",
      "Epoch 100/150\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - accuracy: 0.6488 - loss: 0.1564 - val_accuracy: 0.6272 - val_loss: 0.1656\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step\n",
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# Define the number of hidden units\n",
    "hidden_units = 40\n",
    "\n",
    "# Define your TensorFlow model architecture with explicit input layer\n",
    "input_layer = Input(shape=(13, 21))\n",
    "flattened_layer = Flatten()(input_layer)\n",
    "dense_layer = Dense(hidden_units, activation='sigmoid')(flattened_layer)\n",
    "output_layer = Dense(3, activation='linear')(dense_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Extract features directly from the dense layer\n",
    "feature_extractor = Model(inputs=model.input, outputs=dense_layer)\n",
    "\n",
    "# Extract features\n",
    "features_train = feature_extractor.predict(x_train)\n",
    "features_test = feature_extractor.predict(x_test)\n",
    "\n",
    "# Flatten features\n",
    "features_train = features_train.reshape(features_train.shape[0], -1)\n",
    "features_test = features_test.reshape(features_test.shape[0], -1)\n",
    "\n",
    "# Convert y_train and y_test from one-hot to labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_labels = label_encoder.fit_transform(np.argmax(y_train, axis=1))\n",
    "y_test_labels = label_encoder.transform(np.argmax(y_test, axis=1))\n",
    "\n",
    "# Initialize and train the SVM\n",
    "svm_model = SVC(kernel='linear', C=3, gamma='auto', random_state=42, verbose=True)\n",
    "svm_model.fit(features_train, y_train_labels)  # Ensure y_train is flattened\n",
    "\n",
    "# Predict using the SVM\n",
    "predictions = svm_model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 Score: 62.1\n",
      "Matthews Correlation Coefficients per class: [0.3059098537633514, 0.3176546797501633, 0.3449183493208053]\n",
      "Confusion Matrix:\n",
      " [[196  30 209]\n",
      " [ 72 109 145]\n",
      " [126  68 759]]\n"
     ]
    }
   ],
   "source": [
    "q3_score = accuracy_score(y_test_labels, predictions)\n",
    "mcc_values = [matthews_corrcoef(y_test_labels == i, predictions == i) for i in range(3)]  # Calculate per-class MCC\n",
    "\n",
    "# Print results\n",
    "print(\"Q3 Score:\", round(q3_score * 100, 1))\n",
    "print(\"Matthews Correlation Coefficients per class:\", mcc_values)\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_labels, predictions)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
