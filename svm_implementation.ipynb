{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a sequence of amino acids to a one-hot encoded matrix\n",
    "def one_hot_encode(seq, vocab):\n",
    "    # Create a matrix of zeros, with dimensions len(seq) x len(vocab)\n",
    "    one_hot = np.zeros((len(seq), len(vocab)), dtype=np.float32)\n",
    "    # Iterate over the sequence and set the appropriate elements to 1.0\n",
    "    for i, char in enumerate(seq):\n",
    "        if char in vocab:\n",
    "            one_hot[i, vocab.index(char)] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "# prepare data to enter neural network\n",
    "def prepare_data(filepath, window_size=13):\n",
    "    sequences = []\n",
    "    structures = []\n",
    "    current_seq = []\n",
    "    current_struct = []\n",
    "    processing_sequence = False  # Track when inside a sequence block\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == '<>': # Sequence separator\n",
    "                if processing_sequence: \n",
    "                    if current_seq and current_struct:\n",
    "                        seq_encoded = one_hot_encode(current_seq, aa_vocab) # One-hot encode sequence\n",
    "                        struct_encoded = one_hot_encode(current_struct, structure_vocab) # One-hot encode structure\n",
    "\n",
    "                        # Apply sliding window of determined size\n",
    "                        for i in range(len(seq_encoded) - window_size + 1):\n",
    "                            window = seq_encoded[i:i + window_size]\n",
    "                            label = struct_encoded[i + window_size // 2]\n",
    "                            sequences.append(window)\n",
    "                            structures.append(label)\n",
    "\n",
    "                    current_seq = []\n",
    "                    current_struct = []\n",
    "                processing_sequence = not processing_sequence\n",
    "                continue\n",
    "\n",
    "            elif 'end' in line:  # end of sequence or file\n",
    "                continue  \n",
    "            \n",
    "            # If inside a sequence block, process the sequence\n",
    "            # handles errors in sequence end lines in the dataset\n",
    "            if processing_sequence:\n",
    "                parts = line.split()\n",
    "                if len(parts) != 2:\n",
    "                    continue  \n",
    "                current_seq.append(parts[0])\n",
    "                current_struct.append(parts[1])\n",
    "\n",
    "    return np.array(sequences), np.array(structures)\n",
    "\n",
    "# flatten the sequences for SVM\n",
    "def prepare_data_for_svm(filepath, window_size=13):\n",
    "    sequences, structures = prepare_data(filepath, window_size)\n",
    "    # Flatten the windows for SVM processing\n",
    "    flat_sequences = sequences.reshape(sequences.shape[0], -1)  # Reshape to (number_of_samples, window_size*features_per_aa)\n",
    "    \n",
    "    return flat_sequences, structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example paths, replace with your actual file pathsin\n",
    "train_path = 'Q_and_s_data/protein-secondary-structure.train.txt'\n",
    "#test_path = 'Q_and_s_data/protein-secondary-structure.test.txt'\n",
    "test_path = 'datasets/cb513.txt'\n",
    "\n",
    "\n",
    "x_train, y_train = prepare_data_for_svm(train_path)\n",
    "x_test, y_test = prepare_data_for_svm(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8110, 273) (8110, 3)\n",
      "(11098, 273) (11098, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]\n",
      "Q3 Score (Accuracy): 58.9%\n",
      "Confusion Matrix:\n",
      " [[1967  231 1801]\n",
      " [ 481  654 1259]\n",
      " [ 534  260 3911]]\n",
      "MCC for Helix: 0.38\n",
      "MCC for Sheet: 0.29\n",
      "MCC for Coil: 0.36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Flatten the one-hot encoded labels to a single dimension\n",
    "y_train_flat = np.argmax(y_train, axis=1)\n",
    "y_test_flat = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "# Reshape to (number_of_samples, number_of_features)\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1)) \n",
    "x_test_scaled = scaler.transform(x_test.reshape(x_test.shape[0], -1)) \n",
    "\n",
    "# Initialize and train the SVM using Scikit-learn's SVC\n",
    "svm_model = SVC(kernel='rbf', C=3, random_state=42, verbose=True)\n",
    "# Fit the model\n",
    "svm_model.fit(x_train_scaled, y_train_flat)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = svm_model.predict(x_test_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "q3_score = accuracy_score(y_test_flat, y_pred)\n",
    "cm = confusion_matrix(y_test_flat, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print()\n",
    "print(f\"Q3 Score (Accuracy): {(q3_score * 100):.1f}%\")\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Helper function to calculate MCC for each class\n",
    "def calculate_mcc_for_each_class(y_true, y_pred, num_classes):\n",
    "    mcc_scores = []\n",
    "    for class_id in range(num_classes):\n",
    "        # Create binary labels for the current class\n",
    "        y_true_binary = (y_true == class_id).astype(int)\n",
    "        y_pred_binary = (y_pred == class_id).astype(int)\n",
    "        \n",
    "        # Calculate MCC and append to results\n",
    "        mcc = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "        mcc_scores.append(mcc)\n",
    "    \n",
    "    return mcc_scores\n",
    "\n",
    "# Calculate MCC for each class\n",
    "mcc_scores = calculate_mcc_for_each_class(y_test_flat, y_pred, 3)\n",
    "\n",
    "# Print the MCC for each class\n",
    "class_labels = ['Helix', 'Sheet', 'Coil']\n",
    "for label, mcc in zip(class_labels, mcc_scores):\n",
    "    print(f\"MCC for {label}: {mcc:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio-cwk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
