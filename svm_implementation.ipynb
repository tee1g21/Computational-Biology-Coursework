{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(seq, vocab):\n",
    "    \"\"\"One-hot encode a sequence based on a given vocabulary.\"\"\"\n",
    "    one_hot = np.zeros((len(seq), len(vocab)), dtype=np.float32)\n",
    "    for i, char in enumerate(seq):\n",
    "        if char in vocab:\n",
    "            one_hot[i, vocab.index(char)] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "# Define your vocabularies\n",
    "aa_vocab = 'ACDEFGHIKLMNPQRSTVWY_'  # 20 amino acids + 1 for gap/unknown\n",
    "structure_vocab = 'he_'  # h for helix, e for sheet, _ for coil\n",
    "\n",
    "\n",
    "def prepare_data(filepath, window_size=13):\n",
    "    sequences = []\n",
    "    structures = []\n",
    "    current_seq = []\n",
    "    current_struct = []\n",
    "    processing_sequence = False  # Track when inside a sequence block\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == '<>':  # Toggle processing flag\n",
    "                if processing_sequence:  # We are ending a sequence block\n",
    "                    if current_seq and current_struct:\n",
    "                        seq_encoded = one_hot_encode(current_seq, aa_vocab)\n",
    "                        struct_encoded = one_hot_encode(current_struct, structure_vocab)\n",
    "\n",
    "                        # Apply sliding window\n",
    "                        for i in range(len(seq_encoded) - window_size + 1):\n",
    "                            window = seq_encoded[i:i + window_size]\n",
    "                            label = struct_encoded[i + window_size // 2]\n",
    "                            sequences.append(window)\n",
    "                            structures.append(label)\n",
    "\n",
    "                    current_seq = []\n",
    "                    current_struct = []\n",
    "                processing_sequence = not processing_sequence\n",
    "                continue\n",
    "\n",
    "            elif 'end' in line:  # Generalized handling for any 'end' marker\n",
    "                continue  # Just skip this line, do not end processing sequence\n",
    "\n",
    "            if processing_sequence:\n",
    "                parts = line.split()\n",
    "                if len(parts) != 2:\n",
    "                    continue  # Skip malformed lines or lines that do not fit expected format\n",
    "                current_seq.append(parts[0])\n",
    "                current_struct.append(parts[1])\n",
    "\n",
    "    return np.array(sequences), np.array(structures)\n",
    "\n",
    "\n",
    "def prepare_data_for_svm(filepath, window_size=13):\n",
    "    sequences, structures = prepare_data(filepath, window_size)\n",
    "    # Flatten the windows for SVM processing\n",
    "    flat_sequences = sequences.reshape(sequences.shape[0], -1)  # Reshape to (number_of_samples, window_size*features_per_aa)\n",
    "    \n",
    "    return flat_sequences, structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example paths, replace with your actual file pathsin\n",
    "train_path = 'Q_and_s_data/protein-secondary-structure.train.txt'\n",
    "test_path = 'Q_and_s_data/protein-secondary-structure.test.txt'\n",
    "\n",
    "x_train, y_train = prepare_data_for_svm(train_path)\n",
    "x_test, y_test = prepare_data_for_svm(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8110, 273) (8110, 3)\n",
      "(1714, 273) (1714, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]\n",
      "Q3 Score (Accuracy): 0.6324\n",
      "Confusion Matrix:\n",
      " [[199  25 211]\n",
      " [ 69 111 146]\n",
      " [117  62 774]]\n",
      "MCC for Helix: 0.3254\n",
      "MCC for Sheet: 0.3411\n",
      "MCC for Coil: 0.3598\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "# Assuming x_train and y_train are your input and labels loaded from your function\n",
    "# Flatten the one-hot encoded labels to a single dimension\n",
    "y_train_flat = np.argmax(y_train, axis=1)\n",
    "y_test_flat = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "x_test_scaled = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "\n",
    "# Initialize and train the SVM\n",
    "svm_model = SVC(kernel='rbf', C=2.0, gamma='auto', random_state=42, verbose=True)\n",
    "svm_model.fit(x_train_scaled, y_train_flat)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = svm_model.predict(x_test_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "q3_score = accuracy_score(y_test_flat, y_pred)\n",
    "cm = confusion_matrix(y_test_flat, y_pred)\n",
    "\n",
    "print()\n",
    "print(f\"Q3 Score (Accuracy): {q3_score:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Helper function to calculate MCC for each class\n",
    "def calculate_mcc_for_each_class(y_true, y_pred, num_classes):\n",
    "    mcc_scores = []\n",
    "    for class_id in range(num_classes):\n",
    "        # Create binary labels for the current class\n",
    "        y_true_binary = (y_true == class_id).astype(int)\n",
    "        y_pred_binary = (y_pred == class_id).astype(int)\n",
    "        \n",
    "        # Calculate MCC and append to results\n",
    "        mcc = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "        mcc_scores.append(mcc)\n",
    "    \n",
    "    return mcc_scores\n",
    "\n",
    "# Calculate MCC for each class\n",
    "mcc_scores = calculate_mcc_for_each_class(y_test_flat, y_pred, 3)\n",
    "\n",
    "# Print the MCC for each class\n",
    "class_labels = ['Helix', 'Sheet', 'Coil']\n",
    "for label, mcc in zip(class_labels, mcc_scores):\n",
    "    print(f\"MCC for {label}: {mcc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio-cwk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
