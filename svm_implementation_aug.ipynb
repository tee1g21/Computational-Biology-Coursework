{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define BLOSUM62 Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BLOSUM62 matrix as a dictionary of dictionaries\n",
    "blosum62 = {\n",
    "    'A': {'A': 4, 'R': -1, 'N': -2, 'D': -2, 'C': 0, 'Q': -1, 'E': -1, 'G': 0, 'H': -2, 'I': -1, 'L': -1, 'K': -1, 'M': -1, 'F': -2, 'P': -1, 'S': 1, 'T': 0, 'W': -3, 'Y': -2, 'V': 0},\n",
    "    'R': {'A': -1, 'R': 5, 'N': 0, 'D': -2, 'C': -3, 'Q': 1, 'E': 0, 'G': -2, 'H': 0, 'I': -3, 'L': -2, 'K': 2, 'M': -1, 'F': -3, 'P': -2, 'S': -1, 'T': -1, 'W': -3, 'Y': -2, 'V': -3},\n",
    "    'N': {'A': -2, 'R': 0, 'N': 6, 'D': 1, 'C': -3, 'Q': 0, 'E': 0, 'G': 0, 'H': 1, 'I': -3, 'L': -3, 'K': 0, 'M': -2, 'F': -3, 'P': -2, 'S': 1, 'T': 0, 'W': -4, 'Y': -2, 'V': -3},\n",
    "    'D': {'A': -2, 'R': -2, 'N': 1, 'D': 6, 'C': -3, 'Q': 0, 'E': 2, 'G': -1, 'H': -1, 'I': -3, 'L': -4, 'K': -1, 'M': -3, 'F': -3, 'P': -1, 'S': 0, 'T': -1, 'W': -4, 'Y': -3, 'V': -3},\n",
    "    'C': {'A': 0, 'R': -3, 'N': -3, 'D': -3, 'C': 9, 'Q': -3, 'E': -4, 'G': -3, 'H': -3, 'I': -1, 'L': -1, 'K': -3, 'M': -1, 'F': -2, 'P': -3, 'S': -1, 'T': -1, 'W': -2, 'Y': -2, 'V': -1},\n",
    "    'Q': {'A': -1, 'R': 1, 'N': 0, 'D': 0, 'C': -3, 'Q': 5, 'E': 2, 'G': -2, 'H': 0, 'I': -3, 'L': -2, 'K': 1, 'M': 0, 'F': -3, 'P': -1, 'S': 0, 'T': -1, 'W': -2, 'Y': -1, 'V': -2},\n",
    "    'E': {'A': -1, 'R': 0, 'N': 0, 'D': 2, 'C': -4, 'Q': 2, 'E': 5, 'G': -2, 'H': 0, 'I': -3, 'L': -3, 'K': 1, 'M': -2, 'F': -3, 'P': -1, 'S': 0, 'T': -1, 'W': -3, 'Y': -2, 'V': -2},\n",
    "    'G': {'A': 0, 'R': -2, 'N': 0, 'D': -1, 'C': -3, 'Q': -2, 'E': -2, 'G': 6, 'H': -2, 'I': -4, 'L': -4, 'K': -2, 'M': -3, 'F': -3, 'P': -2, 'S': 0, 'T': -2, 'W': -2, 'Y': -3, 'V': -3},\n",
    "    'H': {'A': -2, 'R': 0, 'N': 1, 'D': -1, 'C': -3, 'Q': 0, 'E': 0, 'G': -2, 'H': 8, 'I': -3, 'L': -3, 'K': -1, 'M': -2, 'F': -1, 'P': -2, 'S': -1, 'T': -2, 'W': -2, 'Y': 2, 'V': -3},\n",
    "    'I': {'A': -1, 'R': -3, 'N': -3, 'D': -3, 'C': -1, 'Q': -3, 'E': -3, 'G': -4, 'H': -3, 'I': 4, 'L': 2, 'K': -3, 'M': 1, 'F': 0, 'P': -3, 'S': -2, 'T': -1, 'W': -3, 'Y': -1, 'V': 3},\n",
    "    'L': {'A': -1, 'R': -2, 'N': -3, 'D': -4, 'C': -1, 'Q': -2, 'E': -3, 'G': -4, 'H': -3, 'I': 2, 'L': 4, 'K': -2, 'M': 2, 'F': 0, 'P': -3, 'S': -2, 'T': -1, 'W': -2, 'Y': -1, 'V': 1},\n",
    "    'K': {'A': -1, 'R': 2, 'N': 0, 'D': -1, 'C': -3, 'Q': 1, 'E': 1, 'G': -2, 'H': -1, 'I': -3, 'L': -2, 'K': 5, 'M': -1, 'F': -3, 'P': -1, 'S': 0, 'T': -1, 'W': -3, 'Y': -2, 'V': -2},\n",
    "    'M': {'A': -1, 'R': -1, 'N': -2, 'D': -3, 'C': -1, 'Q': 0, 'E': -2, 'G': -3, 'H': -2, 'I': 1, 'L': 2, 'K': -1, 'M': 5, 'F': 0, 'P': -2, 'S': -1, 'T': -1, 'W': -1, 'Y': -1, 'V': 1},\n",
    "    'F': {'A': -2, 'R': -3, 'N': -3, 'D': -3, 'C': -2, 'Q': -3, 'E': -3, 'G': -3, 'H': -1, 'I': 0, 'L': 0, 'K': -3, 'M': 0, 'F': 6, 'P': -4, 'S': -2, 'T': -2, 'W': 1, 'Y': 3, 'V': -1},\n",
    "    'P': {'A': -1, 'R': -2, 'N': -2, 'D': -1, 'C': -3, 'Q': -1, 'E': -1, 'G': -2, 'H': -2, 'I': -3, 'L': -3, 'K': -1, 'M': -2, 'F': -4, 'P': 7, 'S': -1, 'T': -1, 'W': -4, 'Y': -3, 'V': -2},\n",
    "    'S': {'A': 1, 'R': -1, 'N': 1, 'D': 0, 'C': -1, 'Q': 0, 'E': 0, 'G': 0, 'H': -1, 'I': -2, 'L': -2, 'K': 0, 'M': -1, 'F': -2, 'P': -1, 'S': 4, 'T': 1, 'W': -3, 'Y': -2, 'V': -2},\n",
    "    'T': {'A': 0, 'R': -1, 'N': 0, 'D': -1, 'C': -1, 'Q': -1, 'E': -1, 'G': -2, 'H': -2, 'I': -1, 'L': -1, 'K': -1, 'M': -1, 'F': -2, 'P': -1, 'S': 1, 'T': 5, 'W': -2, 'Y': -2, 'V': 0},\n",
    "    'W': {'A': -3, 'R': -3, 'N': -4, 'D': -4, 'C': -2, 'Q': -2, 'E': -3, 'G': -2, 'H': -2, 'I': -3, 'L': -2, 'K': -3, 'M': -1, 'F': 1, 'P': -4, 'S': -3, 'T': -2, 'W': 11, 'Y': 2, 'V': -3},\n",
    "    'Y': {'A': -2, 'R': -2, 'N': -2, 'D': -3, 'C': -2, 'Q': -1, 'E': -2, 'G': -3, 'H': 2, 'I': -1, 'L': -1, 'K': -2, 'M': -1, 'F': 3, 'P': -3, 'S': -2, 'T': -2, 'W': 2, 'Y': 7, 'V': -1},\n",
    "    'V': {'A': 0, 'R': -3, 'N': -3, 'D': -3, 'C': -1, 'Q': -2, 'E': -2, 'G': -3, 'H': -3, 'I': 3, 'L': 1, 'K': -2, 'M': 1, 'F': -1, 'P': -2, 'S': -2, 'T': 0, 'W': -3, 'Y': -1, 'V': 4}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conservative substitution dictionary based on a threshold\n",
    "def create_conservative_dict(matrix, threshold=1):\n",
    "    # Create a dictionary to store the conservative substitutions\n",
    "    substitution_dict = {}\n",
    "    # Iterate over the rows of the matrix to find conservative substitutions\n",
    "    for aa1 in matrix:\n",
    "        substitution_dict[aa1] = [aa2 for aa2, score in matrix[aa1].items() if score >= threshold and aa1 != aa2]\n",
    "    return substitution_dict\n",
    "\n",
    "# Create a conservative substitution dictionary based on BLOSUM62 matrix\n",
    "conservative_substitutions_blosum = create_conservative_dict(blosum62)\n",
    "\n",
    "# Function to apply conservative mutations\n",
    "def conservative_mutate_sequence(seq, substitution_dict, mutation_rate=0.05):\n",
    "    # randomly mutate the sequence based on the substitution dictionary and mutation rate\n",
    "    mutated_seq = list(seq)\n",
    "    for i in range(len(seq)):\n",
    "        if random.random() < mutation_rate:\n",
    "            if seq[i] in substitution_dict and substitution_dict[seq[i]]:\n",
    "                mutated_seq[i] = random.choice(substitution_dict[seq[i]])\n",
    "    return ''.join(mutated_seq)\n",
    "\n",
    "# Generate augmented sequences by applying conservative mutations. \n",
    "def augment_sequences(sequences, substitution_dict, num_augmentations=3, mutation_rate=0.05):\n",
    "    augmented_sequences = []\n",
    "    for seq in sequences:\n",
    "        augmented_sequences.append(seq)  # Include the original sequence\n",
    "        for _ in range(num_augmentations):\n",
    "            mutated_seq = conservative_mutate_sequence(seq, substitution_dict, mutation_rate) # Apply conservative mutations\n",
    "            augmented_sequences.append(mutated_seq) # Include the mutated sequence\n",
    "    return augmented_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding function\n",
    "def one_hot_encode(seq, vocab):\n",
    "    one_hot = np.zeros((len(seq), len(vocab)), dtype=np.float32)\n",
    "    for i, char in enumerate(seq):\n",
    "        if char in vocab:\n",
    "            one_hot[i, vocab.index(char)] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "# Define your vocabularies\n",
    "aa_vocab = 'ACDEFGHIKLMNPQRSTVWY_'  # 20 amino acids + 1 for gap/unknown\n",
    "structure_vocab = 'he_'  # h for helix, e for sheet, _ for coil\n",
    "\n",
    "# Modified prepare_data function with augmentation\n",
    "def prepare_data(filepath, window_size=13, augment=False, num_augmentations=1, mutation_rate=0.05):\n",
    "    sequences = []\n",
    "    structures = []\n",
    "    current_seq = []\n",
    "    current_struct = []\n",
    "    processing_sequence = False  \n",
    "    \n",
    "    # Read the file line by line\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == '<>':  \n",
    "                if processing_sequence:  # We are ending a sequence block\n",
    "                    if current_seq and current_struct:\n",
    "                        # Augment sequences if required\n",
    "                        seqs_to_encode = [current_seq]\n",
    "                        if augment:\n",
    "                            # Generate augmented sequences\n",
    "                            seqs_to_encode = augment_sequences([current_seq], conservative_substitutions_blosum, num_augmentations, mutation_rate)\n",
    "\n",
    "                        for augmented_seq in seqs_to_encode:\n",
    "                            seq_encoded = one_hot_encode(augmented_seq, aa_vocab)  #One-hot encode sequence\n",
    "                            struct_encoded = one_hot_encode(current_struct, structure_vocab) # One-hot encode structure\n",
    "\n",
    "                            # Apply sliding window of determined size\n",
    "                            for i in range(len(seq_encoded) - window_size + 1):\n",
    "                                window = seq_encoded[i:i + window_size]\n",
    "                                label = struct_encoded[i + window_size // 2]\n",
    "                                sequences.append(window)\n",
    "                                structures.append(label)\n",
    "\n",
    "                    current_seq = []\n",
    "                    current_struct = []\n",
    "                processing_sequence = not processing_sequence\n",
    "                continue\n",
    "\n",
    "            elif 'end' in line: # end of sequence or file\n",
    "                continue  \n",
    "            \n",
    "             # If inside a sequence block, process the sequence\n",
    "            # handles errors in sequence end lines in the dataset\n",
    "            if processing_sequence:\n",
    "                parts = line.split()\n",
    "                if len(parts) != 2:\n",
    "                    continue  \n",
    "                current_seq.append(parts[0])\n",
    "                current_struct.append(parts[1])\n",
    "\n",
    "    return np.array(sequences), np.array(structures)\n",
    "\n",
    "# flatten the sequences for SVM processing\n",
    "def prepare_data_for_svm(filepath, window_size=13, type='train'):\n",
    "    if type == 'train':\n",
    "        sequences, structures = prepare_data(filepath, augment=True, num_augmentations=3, mutation_rate=0.1)\n",
    "    elif type == 'test':\n",
    "        sequences, structures = prepare_data(filepath, augment=True, num_augmentations=0, mutation_rate=0.0)\n",
    "    \n",
    "    # Flatten the windows for SVM processing\n",
    "    flat_sequences = sequences.reshape(sequences.shape[0], -1)  # Reshape to (number_of_samples, window_size*features_per_aa)\n",
    "    \n",
    "    return flat_sequences, structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# Example paths, replace with your actual file pathsin\n",
    "train_path = 'Q_and_s_data/protein-secondary-structure.train.txt'\n",
    "test_path = 'Q_and_s_data/protein-secondary-structure.test.txt'\n",
    "\n",
    "x_train, y_train = prepare_data_for_svm(train_path, 'train')\n",
    "x_test, y_test = prepare_data_for_svm(test_path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the one-hot encoded labels to a single dimension\n",
    "y_train_flat = np.argmax(y_train, axis=1)\n",
    "y_test_flat = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "x_test_scaled = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "\n",
    "# Initialize and train the SVM\n",
    "svm_model = SVC(kernel='rbf', C=3, gamma='auto', random_state=42, verbose=True)\n",
    "svm_model.fit(x_train_scaled, y_train_flat)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = svm_model.predict(x_test_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "q3_score = accuracy_score(y_test_flat, y_pred)\n",
    "cm = confusion_matrix(y_test_flat, y_pred)\n",
    "\n",
    "print()\n",
    "print(f\"Q3 Score (Accuracy): {(q3_score * 100):.1f}%\")\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Helper function to calculate MCC for each class\n",
    "def calculate_mcc_for_each_class(y_true, y_pred, num_classes):\n",
    "    mcc_scores = []\n",
    "    for class_id in range(num_classes):\n",
    "        # Create binary labels for the current class\n",
    "        y_true_binary = (y_true == class_id).astype(int)\n",
    "        y_pred_binary = (y_pred == class_id).astype(int)\n",
    "        \n",
    "        # Calculate MCC and append to results\n",
    "        mcc = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "        mcc_scores.append(mcc)\n",
    "    \n",
    "    return mcc_scores\n",
    "\n",
    "# Calculate MCC for each class\n",
    "mcc_scores = calculate_mcc_for_each_class(y_test_flat, y_pred, 3)\n",
    "\n",
    "# Print the MCC for each class\n",
    "class_labels = ['Helix', 'Sheet', 'Coil']\n",
    "for label, mcc in zip(class_labels, mcc_scores):\n",
    "    print(f\"MCC for {label}: {mcc:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio-cwk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
